{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Process-Emails\" data-toc-modified-id=\"Process-Emails-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Process Emails</a></span></li><li><span><a href=\"#Create-Bert-Embeddings-for-Emails\" data-toc-modified-id=\"Create-Bert-Embeddings-for-Emails-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Create Bert Embeddings for Emails</a></span></li><li><span><a href=\"#Nearest-Neighbors---Jaccard\" data-toc-modified-id=\"Nearest-Neighbors---Jaccard-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Nearest Neighbors - Manhattan</a></span></li><li><span><a href=\"#Nearest-Neighbors---Cosine\" data-toc-modified-id=\"Nearest-Neighbors---Cosine-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Nearest Neighbors - Cosine</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Doc2Vec</a></span></li><li><span><a href=\"#Precision-&amp;-Recall\" data-toc-modified-id=\"Precision-&amp;-Recall-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Precision &amp; Recall</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "from process_data import process_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_emails(data_folder = os.path.join(\"input_data\", \"results\", \"text\"), \n",
    "               email_text_file = os.path.join(\"input_data\", \"freed_texts.csv\"),\n",
    "               text_filter=\"EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \\\"ZL Technologies, Inc. (http://www.zlti.com)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bert Embeddings for Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/matthewdanielson/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from document_representations_bert import get_doc_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2019 15:03:51 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/matthewdanielson/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/24/2019 15:03:51 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at input_data/bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/24/2019 15:03:51 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/24/2019 15:03:52 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at input_data/bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "12/24/2019 15:03:54 - INFO - root -   embedding chunk number: 0\n",
      "12/24/2019 15:06:15 - INFO - root -   embedding chunk number: 1\n",
      "12/24/2019 15:08:28 - INFO - root -   embedding chunk number: 2\n",
      "12/24/2019 15:10:47 - INFO - root -   embedding chunk number: 3\n",
      "12/24/2019 15:13:07 - INFO - root -   embedding chunk number: 4\n",
      "12/24/2019 15:15:26 - INFO - root -   embedding chunk number: 5\n",
      "12/24/2019 15:16:16 - INFO - root -   Embeddings file successfully written to input_data/freed_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "get_doc_representations(input_email_text_file=os.path.join(\"input_data\", \"freed_texts.csv\"),\n",
    "                        output_embedding_file=os.path.join(\"input_data\", \"freed_embeddings.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors - Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nn_similarity import train_nn, test_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: input_data/freed_manhattan_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train_nn(embeddings_file_path = os.path.join(\"input_data\", \"freed_embeddings.csv\"), \n",
    "         distance_type=\"manhattan\", \n",
    "         file_prefix=\"freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 666\n",
      "1 / 666\n",
      "2 / 666\n",
      "3 / 666\n",
      "4 / 666\n",
      "5 / 666\n",
      "6 / 666\n",
      "7 / 666\n",
      "8 / 666\n",
      "9 / 666\n",
      "10 / 666\n",
      "11 / 666\n",
      "12 / 666\n",
      "13 / 666\n",
      "14 / 666\n",
      "15 / 666\n",
      "16 / 666\n",
      "17 / 666\n",
      "18 / 666\n",
      "19 / 666\n",
      "20 / 666\n",
      "21 / 666\n",
      "22 / 666\n",
      "23 / 666\n",
      "24 / 666\n",
      "25 / 666\n",
      "26 / 666\n",
      "27 / 666\n",
      "28 / 666\n",
      "29 / 666\n",
      "30 / 666\n",
      "31 / 666\n",
      "32 / 666\n",
      "33 / 666\n",
      "34 / 666\n",
      "35 / 666\n",
      "36 / 666\n",
      "37 / 666\n",
      "38 / 666\n",
      "39 / 666\n",
      "40 / 666\n",
      "41 / 666\n",
      "42 / 666\n",
      "43 / 666\n",
      "44 / 666\n",
      "45 / 666\n",
      "46 / 666\n",
      "47 / 666\n",
      "48 / 666\n",
      "49 / 666\n",
      "50 / 666\n",
      "51 / 666\n",
      "52 / 666\n",
      "53 / 666\n",
      "54 / 666\n",
      "55 / 666\n",
      "56 / 666\n",
      "57 / 666\n",
      "58 / 666\n",
      "59 / 666\n",
      "60 / 666\n",
      "61 / 666\n",
      "62 / 666\n",
      "63 / 666\n",
      "64 / 666\n",
      "65 / 666\n",
      "66 / 666\n",
      "67 / 666\n",
      "68 / 666\n",
      "69 / 666\n",
      "70 / 666\n",
      "71 / 666\n",
      "72 / 666\n",
      "73 / 666\n",
      "74 / 666\n",
      "75 / 666\n",
      "76 / 666\n",
      "77 / 666\n",
      "78 / 666\n",
      "79 / 666\n",
      "80 / 666\n",
      "81 / 666\n",
      "82 / 666\n",
      "83 / 666\n",
      "84 / 666\n",
      "85 / 666\n",
      "86 / 666\n",
      "87 / 666\n",
      "88 / 666\n",
      "89 / 666\n",
      "90 / 666\n",
      "91 / 666\n",
      "92 / 666\n",
      "93 / 666\n",
      "94 / 666\n",
      "95 / 666\n",
      "96 / 666\n",
      "97 / 666\n",
      "98 / 666\n",
      "99 / 666\n",
      "100 / 666\n",
      "101 / 666\n",
      "102 / 666\n",
      "103 / 666\n",
      "104 / 666\n",
      "105 / 666\n",
      "106 / 666\n",
      "107 / 666\n",
      "108 / 666\n",
      "109 / 666\n",
      "110 / 666\n",
      "111 / 666\n",
      "112 / 666\n",
      "113 / 666\n",
      "114 / 666\n",
      "115 / 666\n",
      "116 / 666\n",
      "117 / 666\n",
      "118 / 666\n",
      "119 / 666\n",
      "120 / 666\n",
      "121 / 666\n",
      "122 / 666\n",
      "123 / 666\n",
      "124 / 666\n",
      "125 / 666\n",
      "126 / 666\n",
      "127 / 666\n",
      "128 / 666\n",
      "129 / 666\n",
      "130 / 666\n",
      "131 / 666\n",
      "132 / 666\n",
      "133 / 666\n",
      "134 / 666\n",
      "135 / 666\n",
      "136 / 666\n",
      "137 / 666\n",
      "138 / 666\n",
      "139 / 666\n",
      "140 / 666\n",
      "141 / 666\n",
      "142 / 666\n",
      "143 / 666\n",
      "144 / 666\n",
      "145 / 666\n",
      "146 / 666\n",
      "147 / 666\n",
      "148 / 666\n",
      "149 / 666\n",
      "150 / 666\n",
      "151 / 666\n",
      "152 / 666\n",
      "153 / 666\n",
      "154 / 666\n",
      "155 / 666\n",
      "156 / 666\n",
      "157 / 666\n",
      "158 / 666\n",
      "159 / 666\n",
      "160 / 666\n",
      "161 / 666\n",
      "162 / 666\n",
      "163 / 666\n",
      "164 / 666\n",
      "165 / 666\n",
      "166 / 666\n",
      "167 / 666\n",
      "168 / 666\n",
      "169 / 666\n",
      "170 / 666\n",
      "171 / 666\n",
      "172 / 666\n",
      "173 / 666\n",
      "174 / 666\n",
      "175 / 666\n",
      "176 / 666\n",
      "177 / 666\n",
      "178 / 666\n",
      "179 / 666\n",
      "180 / 666\n",
      "181 / 666\n",
      "182 / 666\n",
      "183 / 666\n",
      "184 / 666\n",
      "185 / 666\n",
      "186 / 666\n",
      "187 / 666\n",
      "188 / 666\n",
      "189 / 666\n",
      "190 / 666\n",
      "191 / 666\n",
      "192 / 666\n",
      "193 / 666\n",
      "194 / 666\n",
      "195 / 666\n",
      "196 / 666\n",
      "197 / 666\n",
      "198 / 666\n",
      "199 / 666\n",
      "200 / 666\n",
      "201 / 666\n",
      "202 / 666\n",
      "203 / 666\n",
      "204 / 666\n",
      "205 / 666\n",
      "206 / 666\n",
      "207 / 666\n",
      "208 / 666\n",
      "209 / 666\n",
      "210 / 666\n",
      "211 / 666\n",
      "212 / 666\n",
      "213 / 666\n",
      "214 / 666\n",
      "215 / 666\n",
      "216 / 666\n",
      "217 / 666\n",
      "218 / 666\n",
      "219 / 666\n",
      "220 / 666\n",
      "221 / 666\n",
      "222 / 666\n",
      "223 / 666\n",
      "224 / 666\n",
      "225 / 666\n",
      "226 / 666\n",
      "227 / 666\n",
      "228 / 666\n",
      "229 / 666\n",
      "230 / 666\n",
      "231 / 666\n",
      "232 / 666\n",
      "233 / 666\n",
      "234 / 666\n",
      "235 / 666\n",
      "236 / 666\n",
      "237 / 666\n",
      "238 / 666\n",
      "239 / 666\n",
      "240 / 666\n",
      "241 / 666\n",
      "242 / 666\n",
      "243 / 666\n",
      "244 / 666\n",
      "245 / 666\n",
      "246 / 666\n",
      "247 / 666\n",
      "248 / 666\n",
      "249 / 666\n",
      "250 / 666\n",
      "251 / 666\n",
      "252 / 666\n",
      "253 / 666\n",
      "254 / 666\n",
      "255 / 666\n",
      "256 / 666\n",
      "257 / 666\n",
      "258 / 666\n",
      "259 / 666\n",
      "260 / 666\n",
      "261 / 666\n",
      "262 / 666\n",
      "263 / 666\n",
      "264 / 666\n",
      "265 / 666\n",
      "266 / 666\n",
      "267 / 666\n",
      "268 / 666\n",
      "269 / 666\n",
      "270 / 666\n",
      "271 / 666\n",
      "272 / 666\n",
      "273 / 666\n",
      "274 / 666\n",
      "275 / 666\n",
      "276 / 666\n",
      "277 / 666\n",
      "278 / 666\n",
      "279 / 666\n",
      "280 / 666\n",
      "281 / 666\n",
      "282 / 666\n",
      "283 / 666\n",
      "284 / 666\n",
      "285 / 666\n",
      "286 / 666\n",
      "287 / 666\n",
      "288 / 666\n",
      "289 / 666\n",
      "290 / 666\n",
      "291 / 666\n",
      "292 / 666\n",
      "293 / 666\n",
      "294 / 666\n",
      "295 / 666\n",
      "296 / 666\n",
      "297 / 666\n",
      "298 / 666\n",
      "299 / 666\n",
      "300 / 666\n",
      "301 / 666\n",
      "302 / 666\n",
      "303 / 666\n",
      "304 / 666\n",
      "305 / 666\n",
      "306 / 666\n",
      "307 / 666\n",
      "308 / 666\n",
      "309 / 666\n",
      "310 / 666\n",
      "311 / 666\n",
      "312 / 666\n",
      "313 / 666\n",
      "314 / 666\n",
      "315 / 666\n",
      "316 / 666\n",
      "317 / 666\n",
      "318 / 666\n",
      "319 / 666\n",
      "320 / 666\n",
      "321 / 666\n",
      "322 / 666\n",
      "323 / 666\n",
      "324 / 666\n",
      "325 / 666\n",
      "326 / 666\n",
      "327 / 666\n",
      "328 / 666\n",
      "329 / 666\n",
      "330 / 666\n",
      "331 / 666\n",
      "332 / 666\n",
      "333 / 666\n",
      "334 / 666\n",
      "335 / 666\n",
      "336 / 666\n",
      "337 / 666\n",
      "338 / 666\n",
      "339 / 666\n",
      "340 / 666\n",
      "341 / 666\n",
      "342 / 666\n",
      "343 / 666\n",
      "344 / 666\n",
      "345 / 666\n",
      "346 / 666\n",
      "347 / 666\n",
      "348 / 666\n",
      "349 / 666\n",
      "350 / 666\n",
      "351 / 666\n",
      "352 / 666\n",
      "353 / 666\n",
      "354 / 666\n",
      "355 / 666\n",
      "356 / 666\n",
      "357 / 666\n",
      "358 / 666\n",
      "359 / 666\n",
      "360 / 666\n",
      "361 / 666\n",
      "362 / 666\n",
      "363 / 666\n",
      "364 / 666\n",
      "365 / 666\n",
      "366 / 666\n",
      "367 / 666\n",
      "368 / 666\n",
      "369 / 666\n",
      "370 / 666\n",
      "371 / 666\n",
      "372 / 666\n",
      "373 / 666\n",
      "374 / 666\n",
      "375 / 666\n",
      "376 / 666\n",
      "377 / 666\n",
      "378 / 666\n",
      "379 / 666\n",
      "380 / 666\n",
      "381 / 666\n",
      "382 / 666\n",
      "383 / 666\n",
      "384 / 666\n",
      "385 / 666\n",
      "386 / 666\n",
      "387 / 666\n",
      "388 / 666\n",
      "389 / 666\n",
      "390 / 666\n",
      "391 / 666\n",
      "392 / 666\n",
      "393 / 666\n",
      "394 / 666\n",
      "395 / 666\n",
      "396 / 666\n",
      "397 / 666\n",
      "398 / 666\n",
      "399 / 666\n",
      "400 / 666\n",
      "401 / 666\n",
      "402 / 666\n",
      "403 / 666\n",
      "404 / 666\n",
      "405 / 666\n",
      "406 / 666\n",
      "407 / 666\n",
      "408 / 666\n",
      "409 / 666\n",
      "410 / 666\n",
      "411 / 666\n",
      "412 / 666\n",
      "413 / 666\n",
      "414 / 666\n",
      "415 / 666\n",
      "416 / 666\n",
      "417 / 666\n",
      "418 / 666\n",
      "419 / 666\n",
      "420 / 666\n",
      "421 / 666\n",
      "422 / 666\n",
      "423 / 666\n",
      "424 / 666\n",
      "425 / 666\n",
      "426 / 666\n",
      "427 / 666\n",
      "428 / 666\n",
      "429 / 666\n",
      "430 / 666\n",
      "431 / 666\n",
      "432 / 666\n",
      "433 / 666\n",
      "434 / 666\n",
      "435 / 666\n",
      "436 / 666\n",
      "437 / 666\n",
      "438 / 666\n",
      "439 / 666\n",
      "440 / 666\n",
      "441 / 666\n",
      "442 / 666\n",
      "443 / 666\n",
      "444 / 666\n",
      "445 / 666\n",
      "446 / 666\n",
      "447 / 666\n",
      "448 / 666\n",
      "449 / 666\n",
      "450 / 666\n",
      "451 / 666\n",
      "452 / 666\n",
      "453 / 666\n",
      "454 / 666\n",
      "455 / 666\n",
      "456 / 666\n",
      "457 / 666\n",
      "458 / 666\n",
      "459 / 666\n",
      "460 / 666\n",
      "461 / 666\n",
      "462 / 666\n",
      "463 / 666\n",
      "464 / 666\n",
      "465 / 666\n",
      "466 / 666\n",
      "467 / 666\n",
      "468 / 666\n",
      "469 / 666\n",
      "470 / 666\n",
      "471 / 666\n",
      "472 / 666\n",
      "473 / 666\n",
      "474 / 666\n",
      "475 / 666\n",
      "476 / 666\n",
      "477 / 666\n",
      "478 / 666\n",
      "479 / 666\n",
      "480 / 666\n",
      "481 / 666\n",
      "482 / 666\n",
      "483 / 666\n",
      "484 / 666\n",
      "485 / 666\n",
      "486 / 666\n",
      "487 / 666\n",
      "488 / 666\n",
      "489 / 666\n",
      "490 / 666\n",
      "491 / 666\n",
      "492 / 666\n",
      "493 / 666\n",
      "494 / 666\n",
      "495 / 666\n",
      "496 / 666\n",
      "497 / 666\n",
      "498 / 666\n",
      "499 / 666\n",
      "500 / 666\n",
      "501 / 666\n",
      "502 / 666\n",
      "503 / 666\n",
      "504 / 666\n",
      "505 / 666\n",
      "506 / 666\n",
      "507 / 666\n",
      "508 / 666\n",
      "509 / 666\n",
      "510 / 666\n",
      "511 / 666\n",
      "512 / 666\n",
      "513 / 666\n",
      "514 / 666\n",
      "515 / 666\n",
      "516 / 666\n",
      "517 / 666\n",
      "518 / 666\n",
      "519 / 666\n",
      "520 / 666\n",
      "521 / 666\n",
      "522 / 666\n",
      "523 / 666\n",
      "524 / 666\n",
      "525 / 666\n",
      "526 / 666\n",
      "527 / 666\n",
      "528 / 666\n",
      "529 / 666\n",
      "530 / 666\n",
      "531 / 666\n",
      "532 / 666\n",
      "533 / 666\n",
      "534 / 666\n",
      "535 / 666\n",
      "536 / 666\n",
      "537 / 666\n",
      "538 / 666\n",
      "539 / 666\n",
      "540 / 666\n",
      "541 / 666\n",
      "542 / 666\n",
      "543 / 666\n",
      "544 / 666\n",
      "545 / 666\n",
      "546 / 666\n",
      "547 / 666\n",
      "548 / 666\n",
      "549 / 666\n",
      "550 / 666\n",
      "551 / 666\n",
      "552 / 666\n",
      "553 / 666\n",
      "554 / 666\n",
      "555 / 666\n",
      "556 / 666\n",
      "557 / 666\n",
      "558 / 666\n",
      "559 / 666\n",
      "560 / 666\n",
      "561 / 666\n",
      "562 / 666\n",
      "563 / 666\n",
      "564 / 666\n",
      "565 / 666\n",
      "566 / 666\n",
      "567 / 666\n",
      "568 / 666\n",
      "569 / 666\n",
      "570 / 666\n",
      "571 / 666\n",
      "572 / 666\n",
      "573 / 666\n",
      "574 / 666\n",
      "575 / 666\n",
      "576 / 666\n",
      "577 / 666\n",
      "578 / 666\n",
      "579 / 666\n",
      "580 / 666\n",
      "581 / 666\n",
      "582 / 666\n",
      "583 / 666\n",
      "584 / 666\n",
      "585 / 666\n",
      "586 / 666\n",
      "587 / 666\n",
      "588 / 666\n",
      "589 / 666\n",
      "590 / 666\n",
      "591 / 666\n",
      "592 / 666\n",
      "593 / 666\n",
      "594 / 666\n",
      "595 / 666\n",
      "596 / 666\n",
      "597 / 666\n",
      "598 / 666\n",
      "599 / 666\n",
      "600 / 666\n",
      "601 / 666\n",
      "602 / 666\n",
      "603 / 666\n",
      "604 / 666\n",
      "605 / 666\n",
      "606 / 666\n",
      "607 / 666\n",
      "608 / 666\n",
      "609 / 666\n",
      "610 / 666\n",
      "611 / 666\n",
      "612 / 666\n",
      "613 / 666\n",
      "614 / 666\n",
      "615 / 666\n",
      "616 / 666\n",
      "617 / 666\n",
      "618 / 666\n",
      "619 / 666\n",
      "620 / 666\n",
      "621 / 666\n",
      "622 / 666\n",
      "623 / 666\n",
      "624 / 666\n",
      "625 / 666\n",
      "626 / 666\n",
      "627 / 666\n",
      "628 / 666\n",
      "629 / 666\n",
      "630 / 666\n",
      "631 / 666\n",
      "632 / 666\n",
      "633 / 666\n",
      "634 / 666\n",
      "635 / 666\n",
      "636 / 666\n",
      "637 / 666\n",
      "638 / 666\n",
      "639 / 666\n",
      "640 / 666\n",
      "641 / 666\n",
      "642 / 666\n",
      "643 / 666\n",
      "644 / 666\n",
      "645 / 666\n",
      "646 / 666\n",
      "647 / 666\n",
      "648 / 666\n",
      "649 / 666\n",
      "650 / 666\n",
      "651 / 666\n",
      "652 / 666\n",
      "653 / 666\n",
      "654 / 666\n",
      "655 / 666\n",
      "656 / 666\n",
      "657 / 666\n",
      "658 / 666\n",
      "659 / 666\n",
      "660 / 666\n",
      "661 / 666\n",
      "662 / 666\n",
      "663 / 666\n",
      "664 / 666\n",
      "665 / 666\n",
      "Results saved to: output_data/freed_bert_testing_manhattan.csv\n"
     ]
    }
   ],
   "source": [
    "# test model on same train data\n",
    "test_nn(embeddings_file_path = os.path.join(\"input_data\", \"freed_embeddings.csv\"), \n",
    "        distance_type=\"manhattan\", \n",
    "        file_prefix=\"freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors - Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nn_similarity import train_nn, test_nn, predict_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: input_data/freed_cosine_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train_nn(embeddings_file_path = os.path.join(\"input_data\", \"freed_embeddings.csv\"), \n",
    "         distance_type=\"cosine\",\n",
    "         file_prefix=\"freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 666\n",
      "1 / 666\n",
      "2 / 666\n",
      "3 / 666\n",
      "4 / 666\n",
      "5 / 666\n",
      "6 / 666\n",
      "7 / 666\n",
      "8 / 666\n",
      "9 / 666\n",
      "10 / 666\n",
      "11 / 666\n",
      "12 / 666\n",
      "13 / 666\n",
      "14 / 666\n",
      "15 / 666\n",
      "16 / 666\n",
      "17 / 666\n",
      "18 / 666\n",
      "19 / 666\n",
      "20 / 666\n",
      "21 / 666\n",
      "22 / 666\n",
      "23 / 666\n",
      "24 / 666\n",
      "25 / 666\n",
      "26 / 666\n",
      "27 / 666\n",
      "28 / 666\n",
      "29 / 666\n",
      "30 / 666\n",
      "31 / 666\n",
      "32 / 666\n",
      "33 / 666\n",
      "34 / 666\n",
      "35 / 666\n",
      "36 / 666\n",
      "37 / 666\n",
      "38 / 666\n",
      "39 / 666\n",
      "40 / 666\n",
      "41 / 666\n",
      "42 / 666\n",
      "43 / 666\n",
      "44 / 666\n",
      "45 / 666\n",
      "46 / 666\n",
      "47 / 666\n",
      "48 / 666\n",
      "49 / 666\n",
      "50 / 666\n",
      "51 / 666\n",
      "52 / 666\n",
      "53 / 666\n",
      "54 / 666\n",
      "55 / 666\n",
      "56 / 666\n",
      "57 / 666\n",
      "58 / 666\n",
      "59 / 666\n",
      "60 / 666\n",
      "61 / 666\n",
      "62 / 666\n",
      "63 / 666\n",
      "64 / 666\n",
      "65 / 666\n",
      "66 / 666\n",
      "67 / 666\n",
      "68 / 666\n",
      "69 / 666\n",
      "70 / 666\n",
      "71 / 666\n",
      "72 / 666\n",
      "73 / 666\n",
      "74 / 666\n",
      "75 / 666\n",
      "76 / 666\n",
      "77 / 666\n",
      "78 / 666\n",
      "79 / 666\n",
      "80 / 666\n",
      "81 / 666\n",
      "82 / 666\n",
      "83 / 666\n",
      "84 / 666\n",
      "85 / 666\n",
      "86 / 666\n",
      "87 / 666\n",
      "88 / 666\n",
      "89 / 666\n",
      "90 / 666\n",
      "91 / 666\n",
      "92 / 666\n",
      "93 / 666\n",
      "94 / 666\n",
      "95 / 666\n",
      "96 / 666\n",
      "97 / 666\n",
      "98 / 666\n",
      "99 / 666\n",
      "100 / 666\n",
      "101 / 666\n",
      "102 / 666\n",
      "103 / 666\n",
      "104 / 666\n",
      "105 / 666\n",
      "106 / 666\n",
      "107 / 666\n",
      "108 / 666\n",
      "109 / 666\n",
      "110 / 666\n",
      "111 / 666\n",
      "112 / 666\n",
      "113 / 666\n",
      "114 / 666\n",
      "115 / 666\n",
      "116 / 666\n",
      "117 / 666\n",
      "118 / 666\n",
      "119 / 666\n",
      "120 / 666\n",
      "121 / 666\n",
      "122 / 666\n",
      "123 / 666\n",
      "124 / 666\n",
      "125 / 666\n",
      "126 / 666\n",
      "127 / 666\n",
      "128 / 666\n",
      "129 / 666\n",
      "130 / 666\n",
      "131 / 666\n",
      "132 / 666\n",
      "133 / 666\n",
      "134 / 666\n",
      "135 / 666\n",
      "136 / 666\n",
      "137 / 666\n",
      "138 / 666\n",
      "139 / 666\n",
      "140 / 666\n",
      "141 / 666\n",
      "142 / 666\n",
      "143 / 666\n",
      "144 / 666\n",
      "145 / 666\n",
      "146 / 666\n",
      "147 / 666\n",
      "148 / 666\n",
      "149 / 666\n",
      "150 / 666\n",
      "151 / 666\n",
      "152 / 666\n",
      "153 / 666\n",
      "154 / 666\n",
      "155 / 666\n",
      "156 / 666\n",
      "157 / 666\n",
      "158 / 666\n",
      "159 / 666\n",
      "160 / 666\n",
      "161 / 666\n",
      "162 / 666\n",
      "163 / 666\n",
      "164 / 666\n",
      "165 / 666\n",
      "166 / 666\n",
      "167 / 666\n",
      "168 / 666\n",
      "169 / 666\n",
      "170 / 666\n",
      "171 / 666\n",
      "172 / 666\n",
      "173 / 666\n",
      "174 / 666\n",
      "175 / 666\n",
      "176 / 666\n",
      "177 / 666\n",
      "178 / 666\n",
      "179 / 666\n",
      "180 / 666\n",
      "181 / 666\n",
      "182 / 666\n",
      "183 / 666\n",
      "184 / 666\n",
      "185 / 666\n",
      "186 / 666\n",
      "187 / 666\n",
      "188 / 666\n",
      "189 / 666\n",
      "190 / 666\n",
      "191 / 666\n",
      "192 / 666\n",
      "193 / 666\n",
      "194 / 666\n",
      "195 / 666\n",
      "196 / 666\n",
      "197 / 666\n",
      "198 / 666\n",
      "199 / 666\n",
      "200 / 666\n",
      "201 / 666\n",
      "202 / 666\n",
      "203 / 666\n",
      "204 / 666\n",
      "205 / 666\n",
      "206 / 666\n",
      "207 / 666\n",
      "208 / 666\n",
      "209 / 666\n",
      "210 / 666\n",
      "211 / 666\n",
      "212 / 666\n",
      "213 / 666\n",
      "214 / 666\n",
      "215 / 666\n",
      "216 / 666\n",
      "217 / 666\n",
      "218 / 666\n",
      "219 / 666\n",
      "220 / 666\n",
      "221 / 666\n",
      "222 / 666\n",
      "223 / 666\n",
      "224 / 666\n",
      "225 / 666\n",
      "226 / 666\n",
      "227 / 666\n",
      "228 / 666\n",
      "229 / 666\n",
      "230 / 666\n",
      "231 / 666\n",
      "232 / 666\n",
      "233 / 666\n",
      "234 / 666\n",
      "235 / 666\n",
      "236 / 666\n",
      "237 / 666\n",
      "238 / 666\n",
      "239 / 666\n",
      "240 / 666\n",
      "241 / 666\n",
      "242 / 666\n",
      "243 / 666\n",
      "244 / 666\n",
      "245 / 666\n",
      "246 / 666\n",
      "247 / 666\n",
      "248 / 666\n",
      "249 / 666\n",
      "250 / 666\n",
      "251 / 666\n",
      "252 / 666\n",
      "253 / 666\n",
      "254 / 666\n",
      "255 / 666\n",
      "256 / 666\n",
      "257 / 666\n",
      "258 / 666\n",
      "259 / 666\n",
      "260 / 666\n",
      "261 / 666\n",
      "262 / 666\n",
      "263 / 666\n",
      "264 / 666\n",
      "265 / 666\n",
      "266 / 666\n",
      "267 / 666\n",
      "268 / 666\n",
      "269 / 666\n",
      "270 / 666\n",
      "271 / 666\n",
      "272 / 666\n",
      "273 / 666\n",
      "274 / 666\n",
      "275 / 666\n",
      "276 / 666\n",
      "277 / 666\n",
      "278 / 666\n",
      "279 / 666\n",
      "280 / 666\n",
      "281 / 666\n",
      "282 / 666\n",
      "283 / 666\n",
      "284 / 666\n",
      "285 / 666\n",
      "286 / 666\n",
      "287 / 666\n",
      "288 / 666\n",
      "289 / 666\n",
      "290 / 666\n",
      "291 / 666\n",
      "292 / 666\n",
      "293 / 666\n",
      "294 / 666\n",
      "295 / 666\n",
      "296 / 666\n",
      "297 / 666\n",
      "298 / 666\n",
      "299 / 666\n",
      "300 / 666\n",
      "301 / 666\n",
      "302 / 666\n",
      "303 / 666\n",
      "304 / 666\n",
      "305 / 666\n",
      "306 / 666\n",
      "307 / 666\n",
      "308 / 666\n",
      "309 / 666\n",
      "310 / 666\n",
      "311 / 666\n",
      "312 / 666\n",
      "313 / 666\n",
      "314 / 666\n",
      "315 / 666\n",
      "316 / 666\n",
      "317 / 666\n",
      "318 / 666\n",
      "319 / 666\n",
      "320 / 666\n",
      "321 / 666\n",
      "322 / 666\n",
      "323 / 666\n",
      "324 / 666\n",
      "325 / 666\n",
      "326 / 666\n",
      "327 / 666\n",
      "328 / 666\n",
      "329 / 666\n",
      "330 / 666\n",
      "331 / 666\n",
      "332 / 666\n",
      "333 / 666\n",
      "334 / 666\n",
      "335 / 666\n",
      "336 / 666\n",
      "337 / 666\n",
      "338 / 666\n",
      "339 / 666\n",
      "340 / 666\n",
      "341 / 666\n",
      "342 / 666\n",
      "343 / 666\n",
      "344 / 666\n",
      "345 / 666\n",
      "346 / 666\n",
      "347 / 666\n",
      "348 / 666\n",
      "349 / 666\n",
      "350 / 666\n",
      "351 / 666\n",
      "352 / 666\n",
      "353 / 666\n",
      "354 / 666\n",
      "355 / 666\n",
      "356 / 666\n",
      "357 / 666\n",
      "358 / 666\n",
      "359 / 666\n",
      "360 / 666\n",
      "361 / 666\n",
      "362 / 666\n",
      "363 / 666\n",
      "364 / 666\n",
      "365 / 666\n",
      "366 / 666\n",
      "367 / 666\n",
      "368 / 666\n",
      "369 / 666\n",
      "370 / 666\n",
      "371 / 666\n",
      "372 / 666\n",
      "373 / 666\n",
      "374 / 666\n",
      "375 / 666\n",
      "376 / 666\n",
      "377 / 666\n",
      "378 / 666\n",
      "379 / 666\n",
      "380 / 666\n",
      "381 / 666\n",
      "382 / 666\n",
      "383 / 666\n",
      "384 / 666\n",
      "385 / 666\n",
      "386 / 666\n",
      "387 / 666\n",
      "388 / 666\n",
      "389 / 666\n",
      "390 / 666\n",
      "391 / 666\n",
      "392 / 666\n",
      "393 / 666\n",
      "394 / 666\n",
      "395 / 666\n",
      "396 / 666\n",
      "397 / 666\n",
      "398 / 666\n",
      "399 / 666\n",
      "400 / 666\n",
      "401 / 666\n",
      "402 / 666\n",
      "403 / 666\n",
      "404 / 666\n",
      "405 / 666\n",
      "406 / 666\n",
      "407 / 666\n",
      "408 / 666\n",
      "409 / 666\n",
      "410 / 666\n",
      "411 / 666\n",
      "412 / 666\n",
      "413 / 666\n",
      "414 / 666\n",
      "415 / 666\n",
      "416 / 666\n",
      "417 / 666\n",
      "418 / 666\n",
      "419 / 666\n",
      "420 / 666\n",
      "421 / 666\n",
      "422 / 666\n",
      "423 / 666\n",
      "424 / 666\n",
      "425 / 666\n",
      "426 / 666\n",
      "427 / 666\n",
      "428 / 666\n",
      "429 / 666\n",
      "430 / 666\n",
      "431 / 666\n",
      "432 / 666\n",
      "433 / 666\n",
      "434 / 666\n",
      "435 / 666\n",
      "436 / 666\n",
      "437 / 666\n",
      "438 / 666\n",
      "439 / 666\n",
      "440 / 666\n",
      "441 / 666\n",
      "442 / 666\n",
      "443 / 666\n",
      "444 / 666\n",
      "445 / 666\n",
      "446 / 666\n",
      "447 / 666\n",
      "448 / 666\n",
      "449 / 666\n",
      "450 / 666\n",
      "451 / 666\n",
      "452 / 666\n",
      "453 / 666\n",
      "454 / 666\n",
      "455 / 666\n",
      "456 / 666\n",
      "457 / 666\n",
      "458 / 666\n",
      "459 / 666\n",
      "460 / 666\n",
      "461 / 666\n",
      "462 / 666\n",
      "463 / 666\n",
      "464 / 666\n",
      "465 / 666\n",
      "466 / 666\n",
      "467 / 666\n",
      "468 / 666\n",
      "469 / 666\n",
      "470 / 666\n",
      "471 / 666\n",
      "472 / 666\n",
      "473 / 666\n",
      "474 / 666\n",
      "475 / 666\n",
      "476 / 666\n",
      "477 / 666\n",
      "478 / 666\n",
      "479 / 666\n",
      "480 / 666\n",
      "481 / 666\n",
      "482 / 666\n",
      "483 / 666\n",
      "484 / 666\n",
      "485 / 666\n",
      "486 / 666\n",
      "487 / 666\n",
      "488 / 666\n",
      "489 / 666\n",
      "490 / 666\n",
      "491 / 666\n",
      "492 / 666\n",
      "493 / 666\n",
      "494 / 666\n",
      "495 / 666\n",
      "496 / 666\n",
      "497 / 666\n",
      "498 / 666\n",
      "499 / 666\n",
      "500 / 666\n",
      "501 / 666\n",
      "502 / 666\n",
      "503 / 666\n",
      "504 / 666\n",
      "505 / 666\n",
      "506 / 666\n",
      "507 / 666\n",
      "508 / 666\n",
      "509 / 666\n",
      "510 / 666\n",
      "511 / 666\n",
      "512 / 666\n",
      "513 / 666\n",
      "514 / 666\n",
      "515 / 666\n",
      "516 / 666\n",
      "517 / 666\n",
      "518 / 666\n",
      "519 / 666\n",
      "520 / 666\n",
      "521 / 666\n",
      "522 / 666\n",
      "523 / 666\n",
      "524 / 666\n",
      "525 / 666\n",
      "526 / 666\n",
      "527 / 666\n",
      "528 / 666\n",
      "529 / 666\n",
      "530 / 666\n",
      "531 / 666\n",
      "532 / 666\n",
      "533 / 666\n",
      "534 / 666\n",
      "535 / 666\n",
      "536 / 666\n",
      "537 / 666\n",
      "538 / 666\n",
      "539 / 666\n",
      "540 / 666\n",
      "541 / 666\n",
      "542 / 666\n",
      "543 / 666\n",
      "544 / 666\n",
      "545 / 666\n",
      "546 / 666\n",
      "547 / 666\n",
      "548 / 666\n",
      "549 / 666\n",
      "550 / 666\n",
      "551 / 666\n",
      "552 / 666\n",
      "553 / 666\n",
      "554 / 666\n",
      "555 / 666\n",
      "556 / 666\n",
      "557 / 666\n",
      "558 / 666\n",
      "559 / 666\n",
      "560 / 666\n",
      "561 / 666\n",
      "562 / 666\n",
      "563 / 666\n",
      "564 / 666\n",
      "565 / 666\n",
      "566 / 666\n",
      "567 / 666\n",
      "568 / 666\n",
      "569 / 666\n",
      "570 / 666\n",
      "571 / 666\n",
      "572 / 666\n",
      "573 / 666\n",
      "574 / 666\n",
      "575 / 666\n",
      "576 / 666\n",
      "577 / 666\n",
      "578 / 666\n",
      "579 / 666\n",
      "580 / 666\n",
      "581 / 666\n",
      "582 / 666\n",
      "583 / 666\n",
      "584 / 666\n",
      "585 / 666\n",
      "586 / 666\n",
      "587 / 666\n",
      "588 / 666\n",
      "589 / 666\n",
      "590 / 666\n",
      "591 / 666\n",
      "592 / 666\n",
      "593 / 666\n",
      "594 / 666\n",
      "595 / 666\n",
      "596 / 666\n",
      "597 / 666\n",
      "598 / 666\n",
      "599 / 666\n",
      "600 / 666\n",
      "601 / 666\n",
      "602 / 666\n",
      "603 / 666\n",
      "604 / 666\n",
      "605 / 666\n",
      "606 / 666\n",
      "607 / 666\n",
      "608 / 666\n",
      "609 / 666\n",
      "610 / 666\n",
      "611 / 666\n",
      "612 / 666\n",
      "613 / 666\n",
      "614 / 666\n",
      "615 / 666\n",
      "616 / 666\n",
      "617 / 666\n",
      "618 / 666\n",
      "619 / 666\n",
      "620 / 666\n",
      "621 / 666\n",
      "622 / 666\n",
      "623 / 666\n",
      "624 / 666\n",
      "625 / 666\n",
      "626 / 666\n",
      "627 / 666\n",
      "628 / 666\n",
      "629 / 666\n",
      "630 / 666\n",
      "631 / 666\n",
      "632 / 666\n",
      "633 / 666\n",
      "634 / 666\n",
      "635 / 666\n",
      "636 / 666\n",
      "637 / 666\n",
      "638 / 666\n",
      "639 / 666\n",
      "640 / 666\n",
      "641 / 666\n",
      "642 / 666\n",
      "643 / 666\n",
      "644 / 666\n",
      "645 / 666\n",
      "646 / 666\n",
      "647 / 666\n",
      "648 / 666\n",
      "649 / 666\n",
      "650 / 666\n",
      "651 / 666\n",
      "652 / 666\n",
      "653 / 666\n",
      "654 / 666\n",
      "655 / 666\n",
      "656 / 666\n",
      "657 / 666\n",
      "658 / 666\n",
      "659 / 666\n",
      "660 / 666\n",
      "661 / 666\n",
      "662 / 666\n",
      "663 / 666\n",
      "664 / 666\n",
      "665 / 666\n",
      "Results saved to: output_data/freed_bert_testing_cosine.csv\n"
     ]
    }
   ],
   "source": [
    "# test model on same train data\n",
    "test_nn(embeddings_file_path = os.path.join(\"input_data\", \"freed_embeddings.csv\"), \n",
    "        distance_type=\"cosine\",\n",
    "        file_prefix=\"freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from doc2vec_similarity import train_doc2vec, test_doc2vec,  predict_doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2019 15:21:24 - WARNING - gensim.models.base_any2vec -   consider setting layer size to a multiple of 4 for greater performance\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.doc2vec -   collecting all words and their counts\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.doc2vec -   PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.doc2vec -   collected 16600 word types and 666 unique tags from a corpus of 666 examples and 542032 words\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.word2vec -   effective_min_count=2 retains 12047 unique words (72% of original 16600, drops 4553)\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.word2vec -   effective_min_count=2 leaves 537479 word corpus (99% of original 542032, drops 4553)\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 16600 items\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 34 most-common words\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.word2vec -   downsampling leaves estimated 470134 word corpus (87.5% of prior 537479)\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.base_any2vec -   estimated required memory for 12047 words and 50 dimensions: 11108700 bytes\n",
      "12/24/2019 15:21:24 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "12/24/2019 15:21:26 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 12047 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "12/24/2019 15:21:26 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:26 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:26 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:26 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 542032 raw words (435259 effective words) took 0.3s, 1567831 effective words/s\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 542032 raw words (434945 effective words) took 0.3s, 1593370 effective words/s\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 542032 raw words (435324 effective words) took 0.3s, 1569291 effective words/s\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 542032 raw words (435308 effective words) took 0.3s, 1602755 effective words/s\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:27 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 542032 raw words (434930 effective words) took 0.3s, 1616640 effective words/s\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   EPOCH - 6 : training on 542032 raw words (435281 effective words) took 0.3s, 1593817 effective words/s\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   EPOCH - 7 : training on 542032 raw words (435194 effective words) took 0.3s, 1608106 effective words/s\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   EPOCH - 8 : training on 542032 raw words (435321 effective words) took 0.3s, 1593006 effective words/s\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:28 - INFO - gensim.models.base_any2vec -   EPOCH - 9 : training on 542032 raw words (435407 effective words) took 0.3s, 1633494 effective words/s\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   EPOCH - 10 : training on 542032 raw words (435190 effective words) took 0.3s, 1598543 effective words/s\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   EPOCH - 11 : training on 542032 raw words (435110 effective words) took 0.3s, 1641508 effective words/s\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:29 - INFO - gensim.models.base_any2vec -   EPOCH - 12 : training on 542032 raw words (435215 effective words) took 0.3s, 1617313 effective words/s\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   EPOCH - 13 : training on 542032 raw words (435283 effective words) took 0.3s, 1658784 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   EPOCH - 14 : training on 542032 raw words (435092 effective words) took 0.3s, 1625308 effective words/s\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   EPOCH - 15 : training on 542032 raw words (435162 effective words) took 0.3s, 1619629 effective words/s\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:30 - INFO - gensim.models.base_any2vec -   EPOCH - 16 : training on 542032 raw words (435456 effective words) took 0.3s, 1582933 effective words/s\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   EPOCH - 17 : training on 542032 raw words (435140 effective words) took 0.3s, 1575957 effective words/s\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   EPOCH - 18 : training on 542032 raw words (435062 effective words) took 0.3s, 1506269 effective words/s\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   EPOCH - 19 : training on 542032 raw words (434989 effective words) took 0.3s, 1531063 effective words/s\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:31 - INFO - gensim.models.base_any2vec -   EPOCH - 20 : training on 542032 raw words (435107 effective words) took 0.3s, 1491922 effective words/s\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   EPOCH - 21 : training on 542032 raw words (435256 effective words) took 0.3s, 1503060 effective words/s\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   EPOCH - 22 : training on 542032 raw words (435189 effective words) took 0.3s, 1510414 effective words/s\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:32 - INFO - gensim.models.base_any2vec -   EPOCH - 23 : training on 542032 raw words (435233 effective words) took 0.3s, 1516368 effective words/s\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   EPOCH - 24 : training on 542032 raw words (435193 effective words) took 0.3s, 1574568 effective words/s\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   EPOCH - 25 : training on 542032 raw words (435103 effective words) took 0.3s, 1359041 effective words/s\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:33 - INFO - gensim.models.base_any2vec -   EPOCH - 26 : training on 542032 raw words (435176 effective words) took 0.3s, 1488591 effective words/s\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   EPOCH - 27 : training on 542032 raw words (435127 effective words) took 0.3s, 1470446 effective words/s\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   EPOCH - 28 : training on 542032 raw words (435034 effective words) took 0.3s, 1494516 effective words/s\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   EPOCH - 29 : training on 542032 raw words (435426 effective words) took 0.3s, 1491116 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:34 - INFO - gensim.models.base_any2vec -   EPOCH - 30 : training on 542032 raw words (435127 effective words) took 0.3s, 1523454 effective words/s\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   EPOCH - 31 : training on 542032 raw words (435465 effective words) took 0.3s, 1498119 effective words/s\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   EPOCH - 32 : training on 542032 raw words (435096 effective words) took 0.3s, 1502317 effective words/s\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:35 - INFO - gensim.models.base_any2vec -   EPOCH - 33 : training on 542032 raw words (435475 effective words) took 0.3s, 1411470 effective words/s\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   EPOCH - 34 : training on 542032 raw words (434787 effective words) took 0.3s, 1514192 effective words/s\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   EPOCH - 35 : training on 542032 raw words (435304 effective words) took 0.3s, 1499186 effective words/s\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:36 - INFO - gensim.models.base_any2vec -   EPOCH - 36 : training on 542032 raw words (435032 effective words) took 0.3s, 1495223 effective words/s\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   EPOCH - 37 : training on 542032 raw words (435042 effective words) took 0.3s, 1502176 effective words/s\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   EPOCH - 38 : training on 542032 raw words (435082 effective words) took 0.3s, 1504512 effective words/s\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   EPOCH - 39 : training on 542032 raw words (435056 effective words) took 0.3s, 1480607 effective words/s\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   EPOCH - 40 : training on 542032 raw words (435339 effective words) took 0.3s, 1508598 effective words/s\n",
      "12/24/2019 15:21:37 - INFO - gensim.models.base_any2vec -   training on a 21681280 raw words (17407317 effective words) took 11.5s, 1519947 effective words/s\n",
      "12/24/2019 15:21:37 - INFO - gensim.utils -   saving Doc2Vec object under input_data/freed_doc2vec_model.bin, separately None\n",
      "12/24/2019 15:21:37 - INFO - gensim.utils -   saved input_data/freed_doc2vec_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: input_data/freed_doc2vec_model.bin\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec(model_file_name = os.path.join(\"input_data\", \"freed_doc2vec_model.bin\"), \n",
    "              embeddings_file_path = os.path.join(\"input_data\", \"freed_embeddings.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2019 15:21:39 - INFO - gensim.utils -   loading Doc2Vec object from input_data/freed_doc2vec_model.bin\n",
      "12/24/2019 15:21:39 - INFO - gensim.utils -   loading vocabulary recursively from input_data/freed_doc2vec_model.bin.vocabulary.* with mmap=None\n",
      "12/24/2019 15:21:39 - INFO - gensim.utils -   loading trainables recursively from input_data/freed_doc2vec_model.bin.trainables.* with mmap=None\n",
      "12/24/2019 15:21:39 - INFO - gensim.utils -   loading wv recursively from input_data/freed_doc2vec_model.bin.wv.* with mmap=None\n",
      "12/24/2019 15:21:39 - INFO - gensim.utils -   loading docvecs recursively from input_data/freed_doc2vec_model.bin.docvecs.* with mmap=None\n",
      "12/24/2019 15:21:39 - INFO - gensim.utils -   loaded input_data/freed_doc2vec_model.bin\n",
      "12/24/2019 15:21:39 - INFO - gensim.models.keyedvectors -   precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: input_data/freed_doc2vec_model.bin\n"
     ]
    }
   ],
   "source": [
    "test_doc2vec(embeddings_file_path = os.path.join(\"input_data\", \"freed_embeddings.csv\"), \n",
    "             model_file_name = os.path.join(\"input_data\", \"freed_doc2vec_model.bin\"),\n",
    "             results_path = os.path.join(\"output_data\", \"freed_doc2vec_testing.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from precision_recall import p_r_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Name:  bert_manhattan\n",
      "Precision:  bert_manhattan 1.0\n",
      "Recall:  bert_manhattan 1.0\n",
      "F1 score:  bert_manhattan 1.0\n",
      "\n",
      "Model Name:  bert_cosine\n",
      "Precision:  bert_cosine 1.0\n",
      "Recall:  bert_cosine 1.0\n",
      "F1 score:  bert_cosine 1.0\n",
      "\n",
      "Model Name:  doc2vec\n",
      "Precision:  doc2vec 1.0\n",
      "Recall:  doc2vec 0.6036036036036037\n",
      "F1 score:  doc2vec 0.752808988764045\n"
     ]
    }
   ],
   "source": [
    "p_r_f1_scores(man_results_path       = os.path.join(\"output_data\", \"freed_bert_testing_manhattan.csv\"),\n",
    "              cos_results_path        = os.path.join(\"output_data\", \"freed_bert_testing_cosine.csv\"),\n",
    "              doc2vec_results_path    = os.path.join(\"output_data\", \"freed_doc2vec_testing.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict closest document to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2019 15:25:35 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/matthewdanielson/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/24/2019 15:25:36 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at input_data/bert/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "12/24/2019 15:25:36 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/24/2019 15:25:37 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at input_data/bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from transformers.modeling_bert import BertModel\n",
    "\n",
    "embeddings_file_path = os.path.join(\"input_data\", \"enron_embeddings.csv\")\n",
    "\n",
    "df = pd.read_csv(embeddings_file_path)\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(lambda x: ast.literal_eval(re.sub(\"\\s+\", \", \", re.sub(\"\\[\\s+\", \"[\", x))))\n",
    "\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # # Loading pre-trained model (weights)\n",
    "    # # and putting the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model = BertModel.from_pretrained('bert-base-uncased', cache_dir=os.path.join(\"input_data\", \"bert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"2 of our counterparties are writing letters of complaint.. here's a sample of some of the quotes we have heard from the 10 counterparties we have added in the last 6 months.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'distance': 64.32,\n",
       "  'doc_index': 745,\n",
       "  'text': 'Good morning - these are the levels we will be following today in US,\\n TY,FV, E$, SP:\\t\\t\\t\\t\\t\\t\\n \\t\\tUSH2\\tTYH2\\tFVH2\\tEDM2\\tSPH2\\n target\\t\\t101-26\\t105-05\\t105-19+\\t97.64\\t1157.30\\n target\\t\\t101-09\\t104-24\\t105-12\\t97.60\\t1153.20\\n target\\t\\t100-30\\t104-15\\t105-06\\t97.55\\t1149.10\\n pivot\\t\\t100-24\\t104-12+\\t105-04+\\t97.56\\t1147.40\\n pivot\\t\\t100-14\\t104-04\\t105-00\\t97.52\\t1146.60\\n target\\t\\t100-05\\t103-31+\\t104-28\\t97.50\\t1142.60\\n target\\t\\t99-29\\t103-22+\\t104-23+\\t97.47\\t1140.90\\n target\\t\\t99-20\\t103-17+\\t104-18+\\t97.42\\t1135.00\\n next\\t\\t12/27\\t\\t\\t12/28\\t12/28\\n cycle\\t\\t\\t\\t\\t\\t\\n Good Luck Today\\n Jon and Warren at the CBOT \\n 312-987-5970 or 800-547-3587, 312-347-5061\\n This report has been prepared for informational purposes only. It does not\\n constitute an offer, recommendation or solicitation to buy or sell any\\n securities. It is based on information generally available to the public\\n from sources believed to be reliable. No representation is made that the\\n information is accurate or complete or that any returns indicated will be\\n achieved. Past performance is not indicative of future results. Price and\\n availability are subject to change without notice. Additional information\\n is available upon request. \\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 2: {'distance': 73.857,\n",
       "  'doc_index': 1021,\n",
       "  'text': 'EB2762 !\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 11:08 AM\\n To: Harry Arora/HOU/ECT@ECT@ENRON\\n cc: Steve Crumley/Enron Communications@Enron Communications \\n Subject: Re: Meeting on RSL Capacity Auctions  \\n Harry,\\n We do not have any conference room available between 2:00 and 3:00PM. Can we \\n meet at your office during that time ?\\n Thanks\\n Saji John\\n \\tHarry Arora@ECT\\n \\t02/07/01 10:50 AM\\n \\t\\t To: Saji John/Enron Communications@ENRON COMMUNICATIONS@ENRON\\n \\t\\t cc: \\n \\t\\t Subject: Re: Meeting on RSL Capacity Auctions\\n That sounds good.\\n Harry\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 10:39 AM\\n To: Harry Arora/HOU/ECT@ECT\\n cc:  \\n Subject: Meeting on RSL Capacity Auctions\\n Harry,\\n How about meeting today between 2:00PM and 3:00PM.\\n Thanks\\n Saji John\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 3: {'distance': 73.857,\n",
       "  'doc_index': 830,\n",
       "  'text': 'EB2762 !\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 11:08 AM\\n To: Harry Arora/HOU/ECT@ECT@ENRON\\n cc: Steve Crumley/Enron Communications@Enron Communications \\n Subject: Re: Meeting on RSL Capacity Auctions  \\n Harry,\\n We do not have any conference room available between 2:00 and 3:00PM. Can we \\n meet at your office during that time ?\\n Thanks\\n Saji John\\n \\tHarry Arora@ECT\\n \\t02/07/01 10:50 AM\\n \\t\\t To: Saji John/Enron Communications@ENRON COMMUNICATIONS@ENRON\\n \\t\\t cc: \\n \\t\\t Subject: Re: Meeting on RSL Capacity Auctions\\n That sounds good.\\n Harry\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 10:39 AM\\n To: Harry Arora/HOU/ECT@ECT\\n cc:  \\n Subject: Meeting on RSL Capacity Auctions\\n Harry,\\n How about meeting today between 2:00PM and 3:00PM.\\n Thanks\\n Saji John\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 4: {'distance': 75.875,\n",
       "  'doc_index': 169,\n",
       "  'text': 'Dear Mr. Arora:\\n Once again, thank you for the great opportunity to demonstrate and prove my\\n credentials, interest and desire to work for Enron this summer during the\\n second round of on-campus interview at the Owen Graduate School of\\n Management at Vanderbilt University on January 24th. Although I have not\\n been selected for the Summer Associate position at Enron, I would like to\\n assure you that I am still interested in a full-time position at Enron upon\\n my graduation.\\n Meanwhile, I would greatly appreciate if you provide me with any feedback on\\n my performance during the interview. I am especially interested in your\\n opinion regarding the areas I need to improve.\\n Again, thank you for your time and consideration.\\n Sincerely,\\n Dmitri Villevald\\n Owen MBA 2002\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 5: {'distance': 75.875,\n",
       "  'doc_index': 166,\n",
       "  'text': 'Dear Mr. Arora:\\n Once again, thank you for the great opportunity to demonstrate and prove my\\n credentials, interest and desire to work for Enron this summer during the\\n second round of on-campus interview at the Owen Graduate School of\\n Management at Vanderbilt University on January 24th. Although I have not\\n been selected for the Summer Associate position at Enron, I would like to\\n assure you that I am still interested in a full-time position at Enron upon\\n my graduation.\\n Meanwhile, I would greatly appreciate if you provide me with any feedback on\\n my performance during the interview. I am especially interested in your\\n opinion regarding the areas I need to improve.\\n Again, thank you for your time and consideration.\\n Sincerely,\\n Dmitri Villevald\\n Owen MBA 2002\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nn(document, df, tokenizer, model, closest_docs_threshold=5, distance_type=\"manhattan\", file_prefix =\"enron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'distance': 0.021,\n",
       "  'doc_index': 745,\n",
       "  'text': 'Good morning - these are the levels we will be following today in US,\\n TY,FV, E$, SP:\\t\\t\\t\\t\\t\\t\\n \\t\\tUSH2\\tTYH2\\tFVH2\\tEDM2\\tSPH2\\n target\\t\\t101-26\\t105-05\\t105-19+\\t97.64\\t1157.30\\n target\\t\\t101-09\\t104-24\\t105-12\\t97.60\\t1153.20\\n target\\t\\t100-30\\t104-15\\t105-06\\t97.55\\t1149.10\\n pivot\\t\\t100-24\\t104-12+\\t105-04+\\t97.56\\t1147.40\\n pivot\\t\\t100-14\\t104-04\\t105-00\\t97.52\\t1146.60\\n target\\t\\t100-05\\t103-31+\\t104-28\\t97.50\\t1142.60\\n target\\t\\t99-29\\t103-22+\\t104-23+\\t97.47\\t1140.90\\n target\\t\\t99-20\\t103-17+\\t104-18+\\t97.42\\t1135.00\\n next\\t\\t12/27\\t\\t\\t12/28\\t12/28\\n cycle\\t\\t\\t\\t\\t\\t\\n Good Luck Today\\n Jon and Warren at the CBOT \\n 312-987-5970 or 800-547-3587, 312-347-5061\\n This report has been prepared for informational purposes only. It does not\\n constitute an offer, recommendation or solicitation to buy or sell any\\n securities. It is based on information generally available to the public\\n from sources believed to be reliable. No representation is made that the\\n information is accurate or complete or that any returns indicated will be\\n achieved. Past performance is not indicative of future results. Price and\\n availability are subject to change without notice. Additional information\\n is available upon request. \\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 2: {'distance': 0.022,\n",
       "  'doc_index': 830,\n",
       "  'text': 'EB2762 !\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 11:08 AM\\n To: Harry Arora/HOU/ECT@ECT@ENRON\\n cc: Steve Crumley/Enron Communications@Enron Communications \\n Subject: Re: Meeting on RSL Capacity Auctions  \\n Harry,\\n We do not have any conference room available between 2:00 and 3:00PM. Can we \\n meet at your office during that time ?\\n Thanks\\n Saji John\\n \\tHarry Arora@ECT\\n \\t02/07/01 10:50 AM\\n \\t\\t To: Saji John/Enron Communications@ENRON COMMUNICATIONS@ENRON\\n \\t\\t cc: \\n \\t\\t Subject: Re: Meeting on RSL Capacity Auctions\\n That sounds good.\\n Harry\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 10:39 AM\\n To: Harry Arora/HOU/ECT@ECT\\n cc:  \\n Subject: Meeting on RSL Capacity Auctions\\n Harry,\\n How about meeting today between 2:00PM and 3:00PM.\\n Thanks\\n Saji John\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 3: {'distance': 0.022,\n",
       "  'doc_index': 1021,\n",
       "  'text': 'EB2762 !\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 11:08 AM\\n To: Harry Arora/HOU/ECT@ECT@ENRON\\n cc: Steve Crumley/Enron Communications@Enron Communications \\n Subject: Re: Meeting on RSL Capacity Auctions  \\n Harry,\\n We do not have any conference room available between 2:00 and 3:00PM. Can we \\n meet at your office during that time ?\\n Thanks\\n Saji John\\n \\tHarry Arora@ECT\\n \\t02/07/01 10:50 AM\\n \\t\\t To: Saji John/Enron Communications@ENRON COMMUNICATIONS@ENRON\\n \\t\\t cc: \\n \\t\\t Subject: Re: Meeting on RSL Capacity Auctions\\n That sounds good.\\n Harry\\n From: Saji John@ENRON COMMUNICATIONS on 02/07/2001 10:39 AM\\n To: Harry Arora/HOU/ECT@ECT\\n cc:  \\n Subject: Meeting on RSL Capacity Auctions\\n Harry,\\n How about meeting today between 2:00PM and 3:00PM.\\n Thanks\\n Saji John\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 4: {'distance': 0.027,\n",
       "  'doc_index': 169,\n",
       "  'text': 'Dear Mr. Arora:\\n Once again, thank you for the great opportunity to demonstrate and prove my\\n credentials, interest and desire to work for Enron this summer during the\\n second round of on-campus interview at the Owen Graduate School of\\n Management at Vanderbilt University on January 24th. Although I have not\\n been selected for the Summer Associate position at Enron, I would like to\\n assure you that I am still interested in a full-time position at Enron upon\\n my graduation.\\n Meanwhile, I would greatly appreciate if you provide me with any feedback on\\n my performance during the interview. I am especially interested in your\\n opinion regarding the areas I need to improve.\\n Again, thank you for your time and consideration.\\n Sincerely,\\n Dmitri Villevald\\n Owen MBA 2002\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 5: {'distance': 0.027,\n",
       "  'doc_index': 1534,\n",
       "  'text': 'Dear Mr. Arora:\\n Once again, thank you for the great opportunity to demonstrate and prove my\\n credentials, interest and desire to work for Enron this summer during the\\n second round of on-campus interview at the Owen Graduate School of\\n Management at Vanderbilt University on January 24th. Although I have not\\n been selected for the Summer Associate position at Enron, I would like to\\n assure you that I am still interested in a full-time position at Enron upon\\n my graduation.\\n Meanwhile, I would greatly appreciate if you provide me with any feedback on\\n my performance during the interview. I am especially interested in your\\n opinion regarding the areas I need to improve.\\n Again, thank you for your time and consideration.\\n Sincerely,\\n Dmitri Villevald\\n Owen MBA 2002\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 6: {'distance': 0.027,\n",
       "  'doc_index': 166,\n",
       "  'text': 'Dear Mr. Arora:\\n Once again, thank you for the great opportunity to demonstrate and prove my\\n credentials, interest and desire to work for Enron this summer during the\\n second round of on-campus interview at the Owen Graduate School of\\n Management at Vanderbilt University on January 24th. Although I have not\\n been selected for the Summer Associate position at Enron, I would like to\\n assure you that I am still interested in a full-time position at Enron upon\\n my graduation.\\n Meanwhile, I would greatly appreciate if you provide me with any feedback on\\n my performance during the interview. I am especially interested in your\\n opinion regarding the areas I need to improve.\\n Again, thank you for your time and consideration.\\n Sincerely,\\n Dmitri Villevald\\n Owen MBA 2002\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'},\n",
       " 7: {'distance': 0.027,\n",
       "  'doc_index': 1339,\n",
       "  'text': 'Dear Mr. Arora:\\n Once again, thank you for the great opportunity to demonstrate and prove my\\n credentials, interest and desire to work for Enron this summer during the\\n second round of on-campus interview at the Owen Graduate School of\\n Management at Vanderbilt University on January 24th. Although I have not\\n been selected for the Summer Associate position at Enron, I would like to\\n assure you that I am still interested in a full-time position at Enron upon\\n my graduation.\\n Meanwhile, I would greatly appreciate if you provide me with any feedback on\\n my performance during the interview. I am especially interested in your\\n opinion regarding the areas I need to improve.\\n Again, thank you for your time and consideration.\\n Sincerely,\\n Dmitri Villevald\\n Owen MBA 2002\\n EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc. This Data Set is licensed under a Creative Commons Attribution 3.0 United States License <http://creativecommons.org/licenses/by/3.0/us/> . To provide attribution, please cite to \"ZL Technologies, Inc. (http://www.zlti.com).\"\\n'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nn(document, df, tokenizer, model, closest_docs_threshold=7, distance_type=\"cosine\", file_prefix =\"enron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2019 15:28:47 - INFO - gensim.utils -   loading Doc2Vec object from input_data/enron_doc2vec_model.bin\n",
      "12/24/2019 15:28:47 - INFO - gensim.utils -   loading vocabulary recursively from input_data/enron_doc2vec_model.bin.vocabulary.* with mmap=None\n",
      "12/24/2019 15:28:47 - INFO - gensim.utils -   loading trainables recursively from input_data/enron_doc2vec_model.bin.trainables.* with mmap=None\n",
      "12/24/2019 15:28:47 - INFO - gensim.utils -   loading wv recursively from input_data/enron_doc2vec_model.bin.wv.* with mmap=None\n",
      "12/24/2019 15:28:47 - INFO - gensim.utils -   loading docvecs recursively from input_data/enron_doc2vec_model.bin.docvecs.* with mmap=None\n",
      "12/24/2019 15:28:47 - INFO - gensim.utils -   loaded input_data/enron_doc2vec_model.bin\n",
      "12/24/2019 15:28:47 - INFO - gensim.models.keyedvectors -   precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('enron003_00055.txt', 0.7401232719421387), ('enron003_01531.txt', 0.7396739721298218), ('enron003_00404.txt', 0.7315971255302429), ('enron003_00870.txt', 0.7296277284622192)]\n"
     ]
    }
   ],
   "source": [
    "print(predict_doc2vec(document, model_file_name = os.path.join(\"input_data\", \"enron_doc2vec_model.bin\"), closest_docs_threshold = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "274px",
    "width": "414px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
